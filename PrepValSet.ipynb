{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a7edc29559242b93c8de2387354c06a79d5855a6429ad88e5f2d592a2c3647cd",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# Additional pandas settings\n",
    "pd.set_option('max_row', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/aakritigupta/Desktop/Hackathon 2021/CSV Files'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Set the working directory\n",
    "os.chdir('/Users/aakritigupta/Desktop/Hackathon 2021/CSV Files')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in the data and clean it up \n",
    "def readData(csvFile):\n",
    "  # Read in dataset\n",
    "  df = pd.read_csv(csvFile)\n",
    "\n",
    "  # Extract YEAR out of column and store in new column\n",
    "  df.SERIALNO = df.SERIALNO.map(str)\n",
    "  df['YEAR'] = df.SERIALNO.str[:4]\n",
    "\n",
    "  # Convert YEAR to datetime object and filter dataset to only keep 2016 and 2017 data\n",
    "  df['YEAR'] = pd.to_datetime(df['YEAR']).dt.year\n",
    "#   df2 = df[(df['YEAR'] == 2017)]\n",
    "#   df2 = df[(df['YEAR'] == 2018)]\n",
    "  df2 = df[(df['YEAR'] == 2019)]\n",
    "  \n",
    "  return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop unnecessary columns\n",
    "def dropCols(df):\n",
    "  # Create a list of all columns that need to be dropped\n",
    "  drop_cols = []\n",
    "\n",
    "  # Add person weight indicators to the list\n",
    "  for i in df.columns:\n",
    "    if ('PWGTP' in i) & (len(i) >= 6) or  (i[0]=='F'):\n",
    "      drop_cols.append(i)\n",
    "\n",
    "  # Add the specific columns identified after walking through the dataset manually\n",
    "  drop_cols.extend(['SERIALNO','POBP','RT','DIVISION','SPORDER','PUMA','RELSHIPP','ANC','ANC1P','ANC2P','QTRBIR','RAC2P','RAC3P','OC','RC','ENG','JWRIP','MARHYP','WKWN','YOEP','DECADE','DRIVESP',\"JWAP\",\"JWDP\",'LANP','NAICSP','MIGPUMA','MIGSP','MSP','NOP','PAOC','POWPUMA','POWSP','SCIENGP','SCIENGRLP','SOCP','VPS','CITWP'])\n",
    "\n",
    "  # Drop all columns from the dataframe\n",
    "  df2 = df.drop(columns=drop_cols)\n",
    "\n",
    "  # Drop duplicate rows\n",
    "  df3 = df2.drop_duplicates()\n",
    "\n",
    "  return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values - Numerical and Categorical features\n",
    "def missingVals(df):\n",
    "  # Create a list of numerical columns\n",
    "  Numerical = ['PWGTP','AGEP','INTP','JWMNP','OIP','PAP','RETP','SEMP','SSIP','SSP','WAGP','WKHP','INDP','PERNP','PINCP','POVPIP']\n",
    "\n",
    "  # For all missing values in that list, replace with the mean of the column\n",
    "  for i in Numerical:\n",
    "    df[i].fillna((df[i].mean()), inplace=True)\n",
    "  \n",
    "  # Create a list of categorical columns\n",
    "  Categorical = []\n",
    "\n",
    "  # For all missing values in categorical columns, enter category NoInput and convert dtype\n",
    "  for j in df.columns:\n",
    "    if j not in Numerical:\n",
    "      df[j].fillna('NoInput', inplace=True)\n",
    "      df[j] = df[j].astype('category')\n",
    "      Categorical.append(j)\n",
    "  \n",
    "  return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical variables\n",
    "# def oheCat(df, fileNum):\n",
    "def oheCat(df):\n",
    "  # Extract the year column from the dataframe\n",
    "  date = df['YEAR']\n",
    "  state = df['ST']\n",
    "\n",
    "  # Drop the year column from the dataframe for encoding purposes\n",
    "  df2 = df.drop(columns=['YEAR', 'ST'])\n",
    "\n",
    "  # Create a list of all identified numerical variables\n",
    "  Numerical = ['PWGTP','AGEP','INTP','JWMNP','OIP','PAP','RETP','SEMP','SSIP','SSP','WAGP','WKHP','INDP','PERNP','PINCP','POVPIP']\n",
    "\n",
    "  # Create a list of all categorical columns\n",
    "  cat = []\n",
    "  for i in df2.columns:\n",
    "    if (i not in Numerical):\n",
    "      cat.append(i)\n",
    "  \n",
    "  # One hot encode all of the categorical variables\n",
    "  df3 = pd.get_dummies(df2, prefix=cat)\n",
    "\n",
    "  # Add the YEAR column back into the dataset\n",
    "  df3['YEAR'] = date\n",
    "  df3['ST'] = state\n",
    "\n",
    "  # Write the final dataset back to the Google Drive folder\n",
    "  # filePath = \"/content/gdrive/MyDrive/Hackathon_2021/data/Processed_files/state_\" + str(fileNum) + '.csv'\n",
    "  # df1.to_csv(filePath)\n",
    "  \n",
    "  return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the percentages in each column\n",
    "def agg(df):\n",
    "  # Create the list of numerical features\n",
    "  Numerical = ['PWGTP','AGEP','INTP','JWMNP','OIP','PAP','RETP','SEMP','SSIP','SSP','WAGP','WKHP','INDP','PERNP','PINCP','POVPIP']\n",
    "  cat = []\n",
    "\n",
    "  # Create list of categorical features\n",
    "  for i in df.columns:\n",
    "    if (i not in Numerical) & (i != 'YEAR') & (i != 'ST'):\n",
    "      cat.append(i)\n",
    "    \n",
    "    # Calculate mean of entire column for numerical features\n",
    "    elif (i in Numerical) & (i != 'YEAR'):\n",
    "      df[i] = df[i].mean()\n",
    "\n",
    "  # Calculate a proportion of the categorical columns\n",
    "  for j in cat:\n",
    "    df[j] = df[j].sum()/len(df)\n",
    "\n",
    "  # Remove duplicates\n",
    "  df2 = df.drop_duplicates()\n",
    "  \n",
    "  return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate through all of the files and run each step of the process\n",
    "def allFilesProcess():\n",
    "  lstOfDfs = []\n",
    "  for i, file in enumerate(os.listdir()):\n",
    "    if ('.csv' in file) & (i > 40):\n",
    "      data = readData(file)\n",
    "      data2 = dropCols(data)\n",
    "      data3 = missingVals(data2)\n",
    "      data4 = oheCat(data3)\n",
    "      data5 = agg(data4)\n",
    "      \n",
    "      # Add the df to the list \n",
    "      lstOfDfs.append(data5)\n",
    "      print('Data ' + str(i) + ' has been added to the list')\n",
    "  \n",
    "  dfs = [df.reset_index() for df in lstOfDfs]\n",
    "  dfs_final = pd.concat(dfs, axis=0)\n",
    "  return dfs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data 0 has been added to the list\n",
      "Data 1 has been added to the list\n",
      "Data 2 has been added to the list\n",
      "Data 3 has been added to the list\n",
      "Data 4 has been added to the list\n",
      "Data 5 has been added to the list\n",
      "Data 6 has been added to the list\n",
      "Data 7 has been added to the list\n",
      "Data 8 has been added to the list\n",
      "Data 9 has been added to the list\n",
      "Data 10 has been added to the list\n",
      "Data 12 has been added to the list\n",
      "Data 13 has been added to the list\n",
      "Data 14 has been added to the list\n",
      "Data 15 has been added to the list\n",
      "Data 16 has been added to the list\n",
      "Data 17 has been added to the list\n",
      "Data 18 has been added to the list\n",
      "Data 19 has been added to the list\n",
      "Data 20 has been added to the list\n",
      "CPU times: user 4min 1s, sys: 1min 18s, total: 5min 19s\n",
      "Wall time: 5min 29s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 874)"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "%%time\n",
    "valDf1 = allFilesProcess()\n",
    "valDf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data 21 has been added to the list\n",
      "Data 22 has been added to the list\n",
      "Data 23 has been added to the list\n",
      "Data 24 has been added to the list\n",
      "Data 25 has been added to the list\n",
      "Data 26 has been added to the list\n",
      "Data 27 has been added to the list\n",
      "Data 28 has been added to the list\n",
      "Data 29 has been added to the list\n",
      "Data 30 has been added to the list\n",
      "Data 31 has been added to the list\n",
      "Data 32 has been added to the list\n",
      "Data 33 has been added to the list\n",
      "Data 34 has been added to the list\n",
      "Data 35 has been added to the list\n",
      "Data 36 has been added to the list\n",
      "Data 37 has been added to the list\n",
      "Data 38 has been added to the list\n",
      "Data 39 has been added to the list\n",
      "Data 40 has been added to the list\n",
      "CPU times: user 2min 27s, sys: 32.4 s, total: 2min 59s\n",
      "Wall time: 3min\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 875)"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "%%time\n",
    "valDf2 = allFilesProcess()\n",
    "valDf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data 41 has been added to the list\n",
      "Data 42 has been added to the list\n",
      "Data 43 has been added to the list\n",
      "Data 44 has been added to the list\n",
      "Data 45 has been added to the list\n",
      "Data 46 has been added to the list\n",
      "Data 47 has been added to the list\n",
      "Data 48 has been added to the list\n",
      "Data 49 has been added to the list\n",
      "Data 50 has been added to the list\n",
      "Data 51 has been added to the list\n",
      "Data 52 has been added to the list\n",
      "CPU times: user 1min 45s, sys: 24.1 s, total: 2min 9s\n",
      "Wall time: 2min 10s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12, 872)"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "%%time\n",
    "valDf3 = allFilesProcess()\n",
    "valDf3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine all of the files together\n",
    "def combineFiles(dfa, dfb, dfc):\n",
    "    dfAll = pd.concat([dfa, dfb, dfc], axis=0)\n",
    "\n",
    "    # Drop Puerto Rico from the dataset\n",
    "    dfAll1 = dfAll.loc[(dfAll['ST'] != 72)]\n",
    "\n",
    "    # Drop duplicates from the dataset\n",
    "    dfAll1.drop_duplicates(inplace=True)\n",
    "\n",
    "    return dfAll1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    index      PWGTP       AGEP         INTP      JWMNP         OIP  \\\n",
       "0   35956  20.791180  45.034695  2882.260464  26.265063  654.560010   \n",
       "0  274265  20.668045  42.409024  2760.266599  26.823660  614.216611   \n",
       "0   31501  18.914936  41.606644  3989.927547  19.900990  936.388469   \n",
       "0  473477  19.583745  42.743002  2079.826506  24.689704  568.254658   \n",
       "0  119728  19.923219  42.389087  1494.386997  22.861402  732.928268   \n",
       "\n",
       "         PAP         RETP         SEMP        SSIP  ...  REGION_2  NWAV_4.0  \\\n",
       "0  23.013111  6665.239536  1630.566062  255.055472  ...       NaN       NaN   \n",
       "0  20.872907  4388.770110  1920.396327  246.417240  ...       NaN       NaN   \n",
       "0  37.607523  2918.113149  5001.416988  199.830430  ...       1.0       NaN   \n",
       "0  31.152122  4172.732436  1525.682569  315.128169  ...       1.0  0.000008   \n",
       "0  22.703175  3070.831936  1543.344911  320.637444  ...       NaN       NaN   \n",
       "\n",
       "   OCCP_8510.0  OCCP_8730.0  OCCP_8940.0  RACNUM_6  SFN_3.0  REGION_9  \\\n",
       "0          NaN          NaN          NaN       NaN      NaN       NaN   \n",
       "0          NaN          NaN          NaN       NaN      NaN       NaN   \n",
       "0          NaN          NaN          NaN       NaN      NaN       NaN   \n",
       "0     0.000126     0.000110     0.000160  0.000025      NaN       NaN   \n",
       "0          NaN     0.000066     0.000264       NaN      NaN       NaN   \n",
       "\n",
       "   REGION_1  SFN_4.0  \n",
       "0       NaN      NaN  \n",
       "0       NaN      NaN  \n",
       "0       NaN      NaN  \n",
       "0       NaN      NaN  \n",
       "0       NaN      NaN  \n",
       "\n",
       "[5 rows x 876 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>PWGTP</th>\n      <th>AGEP</th>\n      <th>INTP</th>\n      <th>JWMNP</th>\n      <th>OIP</th>\n      <th>PAP</th>\n      <th>RETP</th>\n      <th>SEMP</th>\n      <th>SSIP</th>\n      <th>...</th>\n      <th>REGION_2</th>\n      <th>NWAV_4.0</th>\n      <th>OCCP_8510.0</th>\n      <th>OCCP_8730.0</th>\n      <th>OCCP_8940.0</th>\n      <th>RACNUM_6</th>\n      <th>SFN_3.0</th>\n      <th>REGION_9</th>\n      <th>REGION_1</th>\n      <th>SFN_4.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35956</td>\n      <td>20.791180</td>\n      <td>45.034695</td>\n      <td>2882.260464</td>\n      <td>26.265063</td>\n      <td>654.560010</td>\n      <td>23.013111</td>\n      <td>6665.239536</td>\n      <td>1630.566062</td>\n      <td>255.055472</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>274265</td>\n      <td>20.668045</td>\n      <td>42.409024</td>\n      <td>2760.266599</td>\n      <td>26.823660</td>\n      <td>614.216611</td>\n      <td>20.872907</td>\n      <td>4388.770110</td>\n      <td>1920.396327</td>\n      <td>246.417240</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>31501</td>\n      <td>18.914936</td>\n      <td>41.606644</td>\n      <td>3989.927547</td>\n      <td>19.900990</td>\n      <td>936.388469</td>\n      <td>37.607523</td>\n      <td>2918.113149</td>\n      <td>5001.416988</td>\n      <td>199.830430</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>473477</td>\n      <td>19.583745</td>\n      <td>42.743002</td>\n      <td>2079.826506</td>\n      <td>24.689704</td>\n      <td>568.254658</td>\n      <td>31.152122</td>\n      <td>4172.732436</td>\n      <td>1525.682569</td>\n      <td>315.128169</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.000008</td>\n      <td>0.000126</td>\n      <td>0.000110</td>\n      <td>0.000160</td>\n      <td>0.000025</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>119728</td>\n      <td>19.923219</td>\n      <td>42.389087</td>\n      <td>1494.386997</td>\n      <td>22.861402</td>\n      <td>732.928268</td>\n      <td>22.703175</td>\n      <td>3070.831936</td>\n      <td>1543.344911</td>\n      <td>320.637444</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000066</td>\n      <td>0.000264</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "completeDf = combineFiles(valDf1, valDf2, valDf3)\n",
    "completeDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(51, 876)"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "completeDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine all of the additional datasets\n",
    "# def addNewData():\n",
    "#     # Read in all of the datasets\n",
    "#     pmh = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/Poor_ Mental_health_Days_2016.csv')\n",
    "#     pmh.rename(columns={'STATE': 'State', 'VALUE':'pmh_Value'}, inplace=True)\n",
    "#     pmh.drop(columns=['RANK'], inplace=True)\n",
    "    \n",
    "#     ob = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/Obesity_2016.csv')\n",
    "#     ob['Value'] = ob['Value'].str.rstrip('%').astype('float') / 100.0\n",
    "#     ob.rename(columns={'Value': 'ob_Value'}, inplace=True)\n",
    "#     ob.drop(columns=['Rank'], inplace=True)\n",
    "    \n",
    "#     isl = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/Insufficient_sleep_2016.csv')\n",
    "#     isl['Value'] = isl['Value'].str.rstrip('%').astype('float') / 100.0\n",
    "#     isl.rename(columns={'Value': 'isl_Value'}, inplace=True)\n",
    "#     isl.drop(columns=['Rank '], inplace=True)\n",
    "    \n",
    "#     fmd = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/Frequent Mental Distress_2016.csv')\n",
    "#     fmd.rename(columns={'Value': 'fmd_Value'}, inplace=True)\n",
    "#     fmd.drop(columns=['Rank '], inplace=True)\n",
    "    \n",
    "#     air = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/Air_Pollution_2016.csv')\n",
    "#     air.rename(columns={'Value': 'air_Value'}, inplace=True)\n",
    "#     air.drop(columns=['Rank'], inplace=True)\n",
    "\n",
    "#     # Join the datasets on the State Name\n",
    "#     lst = [pmh, ob, isl, fmd, air]\n",
    "#     df_complete = reduce(lambda left, right: pd.merge(left, right, on='State'), lst)\n",
    "        \n",
    "#     return df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newData = addNewData()\n",
    "# newData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add in the state initials to the dataset \n",
    "def addStInit(df):\n",
    "    abbr = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/ST_Abbr_Lkp.csv', header=None)\n",
    "    abbr2 = abbr.rename(columns={0:'ST', 1:'State', 2:'LocationAbbr'})\n",
    "    abbr3 = abbr2[(abbr2['ST'] != 72) | (abbr2['ST'] != 11)]\n",
    "\n",
    "    # Join the abbreviations into the dataset\n",
    "    df1 = df.merge(abbr3, how='left', on='ST')\n",
    "\n",
    "    # Add the labels to the dataset - 2017\n",
    "    # label = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/cdc_data_17.csv')\n",
    "\n",
    "    # Add the labels to the dataset - 2018\n",
    "    # label_df = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/cdc_data_1819.csv')\n",
    "    # label_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    # label18 = label_df.loc[label_df['YearStart'] == 2018]\n",
    "\n",
    "    # Add the labels to the dataset - 2019\n",
    "    label_df = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/cdc_data_1819.csv')\n",
    "    label_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    label19 = label_df.loc[label_df['YearStart'] == 2019]\n",
    "\n",
    "    df2 = df1.merge(label19, how='left', on='LocationAbbr')\n",
    "\n",
    "    # Drop Puerto Rico from the dataset\n",
    "    df2.drop((df2[df2['LocationAbbr'] == 'PR'].index) | (df2[df2['LocationAbbr'] == 'DC'].index), inplace=True)\n",
    "\n",
    "    # Add in the additional data features to the dataset\n",
    "    # df3 = df2.merge(addDf, how='left', on='State')\n",
    "\n",
    "    # Drop duplicates from the dataset\n",
    "    df3 = df2.drop_duplicates()\n",
    "\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index_x      PWGTP       AGEP         INTP      JWMNP         OIP  \\\n",
       "0    35956  20.791180  45.034695  2882.260464  26.265063  654.560010   \n",
       "1   274265  20.668045  42.409024  2760.266599  26.823660  614.216611   \n",
       "2    31501  18.914936  41.606644  3989.927547  19.900990  936.388469   \n",
       "3   473477  19.583745  42.743002  2079.826506  24.689704  568.254658   \n",
       "4   119728  19.923219  42.389087  1494.386997  22.861402  732.928268   \n",
       "\n",
       "         PAP         RETP         SEMP        SSIP  ...  RACNUM_6  SFN_3.0  \\\n",
       "0  23.013111  6665.239536  1630.566062  255.055472  ...       NaN      NaN   \n",
       "1  20.872907  4388.770110  1920.396327  246.417240  ...       NaN      NaN   \n",
       "2  37.607523  2918.113149  5001.416988  199.830430  ...       NaN      NaN   \n",
       "3  31.152122  4172.732436  1525.682569  315.128169  ...  0.000025      NaN   \n",
       "4  22.703175  3070.831936  1543.344911  320.637444  ...       NaN      NaN   \n",
       "\n",
       "   REGION_9  REGION_1  SFN_4.0         State  LocationAbbr  index_y  \\\n",
       "0       NaN       NaN      NaN      Delaware            DE     27.0   \n",
       "1       NaN       NaN      NaN       Arizona            AZ     34.0   \n",
       "2       NaN       NaN      NaN  North Dakota            ND      3.0   \n",
       "3       NaN       NaN      NaN          Ohio            OH     38.0   \n",
       "4       NaN       NaN      NaN      Arkansas            AR     36.0   \n",
       "\n",
       "   YearStart  Label  \n",
       "0     2019.0    0.0  \n",
       "1     2019.0    1.0  \n",
       "2     2019.0    0.0  \n",
       "3     2019.0    1.0  \n",
       "4     2019.0    1.0  \n",
       "\n",
       "[5 rows x 881 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_x</th>\n      <th>PWGTP</th>\n      <th>AGEP</th>\n      <th>INTP</th>\n      <th>JWMNP</th>\n      <th>OIP</th>\n      <th>PAP</th>\n      <th>RETP</th>\n      <th>SEMP</th>\n      <th>SSIP</th>\n      <th>...</th>\n      <th>RACNUM_6</th>\n      <th>SFN_3.0</th>\n      <th>REGION_9</th>\n      <th>REGION_1</th>\n      <th>SFN_4.0</th>\n      <th>State</th>\n      <th>LocationAbbr</th>\n      <th>index_y</th>\n      <th>YearStart</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35956</td>\n      <td>20.791180</td>\n      <td>45.034695</td>\n      <td>2882.260464</td>\n      <td>26.265063</td>\n      <td>654.560010</td>\n      <td>23.013111</td>\n      <td>6665.239536</td>\n      <td>1630.566062</td>\n      <td>255.055472</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Delaware</td>\n      <td>DE</td>\n      <td>27.0</td>\n      <td>2019.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>274265</td>\n      <td>20.668045</td>\n      <td>42.409024</td>\n      <td>2760.266599</td>\n      <td>26.823660</td>\n      <td>614.216611</td>\n      <td>20.872907</td>\n      <td>4388.770110</td>\n      <td>1920.396327</td>\n      <td>246.417240</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Arizona</td>\n      <td>AZ</td>\n      <td>34.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31501</td>\n      <td>18.914936</td>\n      <td>41.606644</td>\n      <td>3989.927547</td>\n      <td>19.900990</td>\n      <td>936.388469</td>\n      <td>37.607523</td>\n      <td>2918.113149</td>\n      <td>5001.416988</td>\n      <td>199.830430</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>North Dakota</td>\n      <td>ND</td>\n      <td>3.0</td>\n      <td>2019.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>473477</td>\n      <td>19.583745</td>\n      <td>42.743002</td>\n      <td>2079.826506</td>\n      <td>24.689704</td>\n      <td>568.254658</td>\n      <td>31.152122</td>\n      <td>4172.732436</td>\n      <td>1525.682569</td>\n      <td>315.128169</td>\n      <td>...</td>\n      <td>0.000025</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Ohio</td>\n      <td>OH</td>\n      <td>38.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>119728</td>\n      <td>19.923219</td>\n      <td>42.389087</td>\n      <td>1494.386997</td>\n      <td>22.861402</td>\n      <td>732.928268</td>\n      <td>22.703175</td>\n      <td>3070.831936</td>\n      <td>1543.344911</td>\n      <td>320.637444</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Arkansas</td>\n      <td>AR</td>\n      <td>36.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 881 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "completeDf2 = addStInit(completeDf)\n",
    "completeDf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values on aggregated dataset\n",
    "def missVals(df):\n",
    "    for i in df.columns:\n",
    "        missingVals = df[i].isnull().sum()\n",
    "        if missingVals > 0:\n",
    "            df[i].fillna(0.0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index_x      PWGTP       AGEP         INTP      JWMNP         OIP  \\\n",
       "0    35956  20.791180  45.034695  2882.260464  26.265063  654.560010   \n",
       "1   274265  20.668045  42.409024  2760.266599  26.823660  614.216611   \n",
       "2    31501  18.914936  41.606644  3989.927547  19.900990  936.388469   \n",
       "3   473477  19.583745  42.743002  2079.826506  24.689704  568.254658   \n",
       "4   119728  19.923219  42.389087  1494.386997  22.861402  732.928268   \n",
       "\n",
       "         PAP         RETP         SEMP        SSIP  ...  RACNUM_6  SFN_3.0  \\\n",
       "0  23.013111  6665.239536  1630.566062  255.055472  ...  0.000000      0.0   \n",
       "1  20.872907  4388.770110  1920.396327  246.417240  ...  0.000000      0.0   \n",
       "2  37.607523  2918.113149  5001.416988  199.830430  ...  0.000000      0.0   \n",
       "3  31.152122  4172.732436  1525.682569  315.128169  ...  0.000025      0.0   \n",
       "4  22.703175  3070.831936  1543.344911  320.637444  ...  0.000000      0.0   \n",
       "\n",
       "   REGION_9  REGION_1  SFN_4.0         State  LocationAbbr  index_y  \\\n",
       "0       0.0       0.0      0.0      Delaware            DE     27.0   \n",
       "1       0.0       0.0      0.0       Arizona            AZ     34.0   \n",
       "2       0.0       0.0      0.0  North Dakota            ND      3.0   \n",
       "3       0.0       0.0      0.0          Ohio            OH     38.0   \n",
       "4       0.0       0.0      0.0      Arkansas            AR     36.0   \n",
       "\n",
       "   YearStart  Label  \n",
       "0     2019.0    0.0  \n",
       "1     2019.0    1.0  \n",
       "2     2019.0    0.0  \n",
       "3     2019.0    1.0  \n",
       "4     2019.0    1.0  \n",
       "\n",
       "[5 rows x 881 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index_x</th>\n      <th>PWGTP</th>\n      <th>AGEP</th>\n      <th>INTP</th>\n      <th>JWMNP</th>\n      <th>OIP</th>\n      <th>PAP</th>\n      <th>RETP</th>\n      <th>SEMP</th>\n      <th>SSIP</th>\n      <th>...</th>\n      <th>RACNUM_6</th>\n      <th>SFN_3.0</th>\n      <th>REGION_9</th>\n      <th>REGION_1</th>\n      <th>SFN_4.0</th>\n      <th>State</th>\n      <th>LocationAbbr</th>\n      <th>index_y</th>\n      <th>YearStart</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35956</td>\n      <td>20.791180</td>\n      <td>45.034695</td>\n      <td>2882.260464</td>\n      <td>26.265063</td>\n      <td>654.560010</td>\n      <td>23.013111</td>\n      <td>6665.239536</td>\n      <td>1630.566062</td>\n      <td>255.055472</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Delaware</td>\n      <td>DE</td>\n      <td>27.0</td>\n      <td>2019.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>274265</td>\n      <td>20.668045</td>\n      <td>42.409024</td>\n      <td>2760.266599</td>\n      <td>26.823660</td>\n      <td>614.216611</td>\n      <td>20.872907</td>\n      <td>4388.770110</td>\n      <td>1920.396327</td>\n      <td>246.417240</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Arizona</td>\n      <td>AZ</td>\n      <td>34.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31501</td>\n      <td>18.914936</td>\n      <td>41.606644</td>\n      <td>3989.927547</td>\n      <td>19.900990</td>\n      <td>936.388469</td>\n      <td>37.607523</td>\n      <td>2918.113149</td>\n      <td>5001.416988</td>\n      <td>199.830430</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>North Dakota</td>\n      <td>ND</td>\n      <td>3.0</td>\n      <td>2019.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>473477</td>\n      <td>19.583745</td>\n      <td>42.743002</td>\n      <td>2079.826506</td>\n      <td>24.689704</td>\n      <td>568.254658</td>\n      <td>31.152122</td>\n      <td>4172.732436</td>\n      <td>1525.682569</td>\n      <td>315.128169</td>\n      <td>...</td>\n      <td>0.000025</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Ohio</td>\n      <td>OH</td>\n      <td>38.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>119728</td>\n      <td>19.923219</td>\n      <td>42.389087</td>\n      <td>1494.386997</td>\n      <td>22.861402</td>\n      <td>732.928268</td>\n      <td>22.703175</td>\n      <td>3070.831936</td>\n      <td>1543.344911</td>\n      <td>320.637444</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Arkansas</td>\n      <td>AR</td>\n      <td>36.0</td>\n      <td>2019.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 881 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "valData3 = missVals(completeDf2)\n",
    "valData3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in the dataset\n",
    "def filterDf(df):\n",
    "    # Read in the dataset\n",
    "    fs = pd.read_csv('https://raw.githubusercontent.com/aagupta/MentalAid/main/stats_train.csv')\n",
    "    \n",
    "    # Drop the unnecessary columns\n",
    "    fs2 = fs.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Label_y'])\n",
    "    fsLabel = fs['Label_y']\n",
    "    state = df['State']\n",
    "\n",
    "    # Create a list of important features\n",
    "    fsCols = fs2.columns.tolist()\n",
    "\n",
    "    # Subset the dataframe\n",
    "    fs3 = df[fsCols]\n",
    "\n",
    "    # Write the dataset\n",
    "    # fs3.to_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/Validation_2017.csv')\n",
    "    fs3.to_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/Validation_2019.csv')\n",
    "\n",
    "    return (fs3, fsLabel, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   JWTRNS_8.0    HISP_2    HISP_4   HISP_21   HISP_24  OCCP_6240.0  \\\n",
       "0    0.000649  0.029075  0.001081  0.000216  0.001729     0.000216   \n",
       "1    0.001691  0.214272  0.002003  0.000341  0.009035     0.000696   \n",
       "2    0.000503  0.022650  0.000378  0.000000  0.002643     0.000378   \n",
       "3    0.000396  0.013955  0.000632  0.000388  0.002039     0.000472   \n",
       "4    0.000660  0.041559  0.000528  0.000264  0.002905     0.000429   \n",
       "\n",
       "   OCCP_9830.0   RAC1P_3   RAC1P_7   RAC1P_9  ...         OIP        PAP  \\\n",
       "0     0.001513  0.003351  0.000108  0.026157  ...  654.560010  23.013111   \n",
       "1     0.000852  0.057734  0.001776  0.036737  ...  614.216611  20.872907   \n",
       "2     0.001133  0.045678  0.001007  0.019504  ...  936.388469  37.607523   \n",
       "3     0.000194  0.000986  0.000312  0.023688  ...  568.254658  31.152122   \n",
       "4     0.000363  0.004093  0.001584  0.023536  ...  732.928268  22.703175   \n",
       "\n",
       "          RETP         SEMP        SSIP          SSP          WAGP  \\\n",
       "0  6665.239536  1630.566062  255.055472  4798.047151  31282.536561   \n",
       "1  4388.770110  1920.396327  246.417240  4051.408314  27531.802771   \n",
       "2  2918.113149  5001.416988  199.830430  3547.653769  28975.946354   \n",
       "3  4172.732436  1525.682569  315.128169  3681.867809  28113.972425   \n",
       "4  3070.831936  1543.344911  320.637444  4040.786472  22274.849581   \n",
       "\n",
       "          PERNP         PINCP      POVPIP  \n",
       "0  33324.831504  48191.278366  355.059495  \n",
       "1  29895.433271  41534.150897  317.284379  \n",
       "2  34423.107293  45606.884230  343.112497  \n",
       "3  30071.412929  40488.616693  321.655711  \n",
       "4  24194.139714  33500.468823  283.596783  \n",
       "\n",
       "[5 rows x 37 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>JWTRNS_8.0</th>\n      <th>HISP_2</th>\n      <th>HISP_4</th>\n      <th>HISP_21</th>\n      <th>HISP_24</th>\n      <th>OCCP_6240.0</th>\n      <th>OCCP_9830.0</th>\n      <th>RAC1P_3</th>\n      <th>RAC1P_7</th>\n      <th>RAC1P_9</th>\n      <th>...</th>\n      <th>OIP</th>\n      <th>PAP</th>\n      <th>RETP</th>\n      <th>SEMP</th>\n      <th>SSIP</th>\n      <th>SSP</th>\n      <th>WAGP</th>\n      <th>PERNP</th>\n      <th>PINCP</th>\n      <th>POVPIP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000649</td>\n      <td>0.029075</td>\n      <td>0.001081</td>\n      <td>0.000216</td>\n      <td>0.001729</td>\n      <td>0.000216</td>\n      <td>0.001513</td>\n      <td>0.003351</td>\n      <td>0.000108</td>\n      <td>0.026157</td>\n      <td>...</td>\n      <td>654.560010</td>\n      <td>23.013111</td>\n      <td>6665.239536</td>\n      <td>1630.566062</td>\n      <td>255.055472</td>\n      <td>4798.047151</td>\n      <td>31282.536561</td>\n      <td>33324.831504</td>\n      <td>48191.278366</td>\n      <td>355.059495</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001691</td>\n      <td>0.214272</td>\n      <td>0.002003</td>\n      <td>0.000341</td>\n      <td>0.009035</td>\n      <td>0.000696</td>\n      <td>0.000852</td>\n      <td>0.057734</td>\n      <td>0.001776</td>\n      <td>0.036737</td>\n      <td>...</td>\n      <td>614.216611</td>\n      <td>20.872907</td>\n      <td>4388.770110</td>\n      <td>1920.396327</td>\n      <td>246.417240</td>\n      <td>4051.408314</td>\n      <td>27531.802771</td>\n      <td>29895.433271</td>\n      <td>41534.150897</td>\n      <td>317.284379</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000503</td>\n      <td>0.022650</td>\n      <td>0.000378</td>\n      <td>0.000000</td>\n      <td>0.002643</td>\n      <td>0.000378</td>\n      <td>0.001133</td>\n      <td>0.045678</td>\n      <td>0.001007</td>\n      <td>0.019504</td>\n      <td>...</td>\n      <td>936.388469</td>\n      <td>37.607523</td>\n      <td>2918.113149</td>\n      <td>5001.416988</td>\n      <td>199.830430</td>\n      <td>3547.653769</td>\n      <td>28975.946354</td>\n      <td>34423.107293</td>\n      <td>45606.884230</td>\n      <td>343.112497</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000396</td>\n      <td>0.013955</td>\n      <td>0.000632</td>\n      <td>0.000388</td>\n      <td>0.002039</td>\n      <td>0.000472</td>\n      <td>0.000194</td>\n      <td>0.000986</td>\n      <td>0.000312</td>\n      <td>0.023688</td>\n      <td>...</td>\n      <td>568.254658</td>\n      <td>31.152122</td>\n      <td>4172.732436</td>\n      <td>1525.682569</td>\n      <td>315.128169</td>\n      <td>3681.867809</td>\n      <td>28113.972425</td>\n      <td>30071.412929</td>\n      <td>40488.616693</td>\n      <td>321.655711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000660</td>\n      <td>0.041559</td>\n      <td>0.000528</td>\n      <td>0.000264</td>\n      <td>0.002905</td>\n      <td>0.000429</td>\n      <td>0.000363</td>\n      <td>0.004093</td>\n      <td>0.001584</td>\n      <td>0.023536</td>\n      <td>...</td>\n      <td>732.928268</td>\n      <td>22.703175</td>\n      <td>3070.831936</td>\n      <td>1543.344911</td>\n      <td>320.637444</td>\n      <td>4040.786472</td>\n      <td>22274.849581</td>\n      <td>24194.139714</td>\n      <td>33500.468823</td>\n      <td>283.596783</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "(valData4, label, state) = filterDf(valData3)\n",
    "valData4.head()"
   ]
  },
  {
   "source": [
    "## Run code after predictions have been made"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add label back into the dataset to compare\n",
    "def addLabel(lst, stateLst):\n",
    "    # Read in the dataset\n",
    "    # pred = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/Predictions_2017.csv')\n",
    "    # pred = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/Predictions_2018.csv')\n",
    "    pred = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/Predictions_2019.csv')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    pred.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    # Join the label to the dataset and the state column\n",
    "    pred['Label'] = lst\n",
    "\n",
    "    # Add a condition if values don't match\n",
    "    pred['Match?'] = np.where(pred['Predictions'] != pred['Label'], 'False', 'True')\n",
    "\n",
    "    # Add in the states to the dataset\n",
    "    pred['State'] = state\n",
    "\n",
    "    # Write df to csv\n",
    "    # pred.to_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/PredWLabel_2017.csv')\n",
    "    # pred.to_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/PredWLabel_2018.csv')\n",
    "    pred.to_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/PredWLabel_2019.csv')\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 11.7 ms, sys: 2.47 ms, total: 14.2 ms\nWall time: 13.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = addLabel(label, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('/Users/aakritigupta/Desktop/Hackathon 2021/MentSea/MentSea/PredWLabel_2018.csv')\n",
    "for i in x.columns:\n",
    "    if '_Value' in i:\n",
    "        print(i)"
   ]
  }
 ]
}